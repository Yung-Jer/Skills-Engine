{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "import aspose.words as aw\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Tokenize words only with the whitespace rule\n",
    "# N-grams will no longer be treated as 'N' and '-grams'\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = txt.lower()\n",
    "    # Remove non-English charcters\n",
    "    txt = re.sub('[^a-zA-Z]', ' ', txt)\n",
    "    txt = re.sub('http\\S+\\s*', ' ', txt)  # remove URLs\n",
    "    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc\n",
    "    txt = re.sub('#\\S+', '', txt)  # remove hashtags\n",
    "    txt = re.sub('@\\S+', '  ', txt)  # remove mentions\n",
    "    txt = re.sub('\\s+', ' ', txt)  # remove extra whitespace\n",
    "\n",
    "    # tokenize word\n",
    "    txt = nlp(txt)\n",
    "\n",
    "    # remove stop words and lemmatization\n",
    "    txt = [token.lemma_ for token in txt if not token.is_stop]\n",
    "\n",
    "    return ' '.join(txt)\n",
    "\n",
    "# STOP = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def topic_modelling(resume_text, num_words=10, min_prob = 10**-2):\n",
    "    doc_clean = [resume_text.split()]   \n",
    "    \n",
    "    # term dictionary\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_mat = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    # latent dirichlet allocation model \n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    ldamodel = Lda(doc_term_mat, num_topics=1, id2word = dictionary, passes=50)\n",
    "    \n",
    "    # Return only the topic words that have the probability of larger than .01\n",
    "    return [token for token, prob in ldamodel.show_topic(0, topn=num_words) if prob > min_prob ]\n",
    "\n",
    "def n_grams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the skills from EMSI Skills API dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>infoUrl</th>\n",
       "      <th>name</th>\n",
       "      <th>type.id</th>\n",
       "      <th>type.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KS126XS6CQCFGC3NG79X</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XS6CQC...</td>\n",
       "      <td>.net assemblies</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ES50D03AC9CFC1A0BC93</td>\n",
       "      <td>https://skills.emsidata.com/skills/ES50D03AC9C...</td>\n",
       "      <td>.net development</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>KS1200B62W5ZF38RJ7TD</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS1200B62W5...</td>\n",
       "      <td>.net framework</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KS126XW78QJCF4TRV2X7</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XW78QJ...</td>\n",
       "      <td>.net framework 1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>KS126XY68BNKXSBSLPYS</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XY68BN...</td>\n",
       "      <td>.net framework 3</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    id  \\\n",
       "0           0  KS126XS6CQCFGC3NG79X   \n",
       "1           1  ES50D03AC9CFC1A0BC93   \n",
       "2           2  KS1200B62W5ZF38RJ7TD   \n",
       "3           3  KS126XW78QJCF4TRV2X7   \n",
       "4           4  KS126XY68BNKXSBSLPYS   \n",
       "\n",
       "                                             infoUrl              name  \\\n",
       "0  https://skills.emsidata.com/skills/KS126XS6CQC...   .net assemblies   \n",
       "1  https://skills.emsidata.com/skills/ES50D03AC9C...  .net development   \n",
       "2  https://skills.emsidata.com/skills/KS1200B62W5...    .net framework   \n",
       "3  https://skills.emsidata.com/skills/KS126XW78QJ...  .net framework 1   \n",
       "4  https://skills.emsidata.com/skills/KS126XY68BN...  .net framework 3   \n",
       "\n",
       "  type.id          type.name  \n",
       "0     ST1  Specialized Skill  \n",
       "1     ST1  Specialized Skill  \n",
       "2     ST1  Specialized Skill  \n",
       "3     ST1  Specialized Skill  \n",
       "4     ST1  Specialized Skill  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_excel(\"all_skills_emsi.xlsx\")\n",
    "# remove all the additional descriptions in round brackets\n",
    "skills['name'] = skills['name'].apply(lambda x: re.sub(\"\\s\\(.*?\\)\",\"\",x))\n",
    "skills['name'] = skills['name'].apply(lambda x: x.lower())\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the skills into a big hashset\n",
    "skills_api = set(skills['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text, skills_api, clean=True):\n",
    "    if clean == True:\n",
    "        resume_text = preprocess(resume_text)\n",
    "        \n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # all the resume skills will be saved here\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if nlp(token)[0].tag_ != 'VBN':\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for noun_chunks (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for bigrams that SpaCy missed in the noun_chuncks\n",
    "    for token in n_grams(tokens, 2):\n",
    "        token = token.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for trigrams that SpaCy missed in the noun_chuncks\n",
    "    for token in n_grams(tokens, 3):\n",
    "        token = token.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for LDA topic modelling result\n",
    "    skillset.extend(topic_modelling(resume_text))\n",
    "    \n",
    "    # get only the unique skills in lowercase\n",
    "    skillset = set([i for i in set([i.lower() for i in skillset])])\n",
    "\n",
    "    return skillset.intersection(skills_api)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read any resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = aw.Document(\"34740556.pdf\")\n",
    "resume_string = resume.to_string(aw.SaveFormat.TEXT).split('\\r\\n')\n",
    "resume_string = ' '.join(resume_string[1:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SENIOR HR BUSINESS PARTNER Summary Human Resources Professional with 8 years of experience in human resources and recruiting. Expertise in Employee Relations and Recruiting. Highly driven to achieve company goals. Highlights · HUMAN RESOURCES MANAGER · Extensive background in HR Business Partner affairs, including experience in, staff development, mediation, conflict resolution, benefits and compensation, HR records management, HR policies development and legal compliance. · Demonstrated success in negotiating win-win compromises, developing teambuilding programs, and writing policies, job descriptions and management reports. · HR SKILLS · Employment Law · FMLA/ADA/EEO/WC · HR Policies & Procedures *Staff Recruitment & Retention · Employee Relations · Benefits Administration *Orientation & On-Boarding  Training & Development  Organizational Development  MS Office (Word, Excel, PowerPoint, Access, Outlook) Experience Senior HR Business Partner  August 2013 to Current Company Name ï¼ Cit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_string[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada',\n",
       " 'business administration',\n",
       " 'coaching',\n",
       " 'conflict resolution',\n",
       " 'environmental quality',\n",
       " 'labor law',\n",
       " 'management',\n",
       " 'mediation',\n",
       " 'organizational development',\n",
       " 'policy development',\n",
       " 'poultry',\n",
       " 'reduction',\n",
       " 'session',\n",
       " 'strong work ethic',\n",
       " 'teamwork',\n",
       " 'workplace safety'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(resume_text=resume_string, skills_api=skills_api, clean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is actually assuming all the skills have the same level of experience.\n",
    "\n",
    "We are going to identify each section using the date range in resume, then label all the skills within the section with the year of experience computed from the date range in resume. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date range identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[958, 969, datetime.date(2013, 8, 1)],\n",
       " [973, 980, datetime.date(2023, 2, 14)],\n",
       " [1066, 1080, datetime.date(2013, 9, 1)],\n",
       " [1851, 1858, datetime.date(2023, 2, 14)],\n",
       " [1969, 1980, datetime.date(2010, 8, 1)],\n",
       " [1984, 1995, datetime.date(2013, 8, 1)],\n",
       " [3608, 3619, datetime.date(2008, 8, 1)],\n",
       " [3623, 3634, datetime.date(2010, 8, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map all the possible cases of date format in resume\n",
    "# Date formats include:\n",
    "# Aug 2022\n",
    "# 05/2023\n",
    "# Present\n",
    "# Current\n",
    "# 05 / 2023\n",
    "# Aug / 2023\n",
    "# 03-2023\n",
    "# 03 - 2023\n",
    "pattern = r'(((Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|June?|July?|Aug(ust)?|Sep(tember)?|Nov(ember)?|Dec(ember)?)|(\\d{1,2}\\s?\\/){0,2}|(\\d{1,2}\\s?\\-){0,2})\\s?[-/ ]?\\s?\\d{4}?)|\\bPresent\\b|\\bpresent\\b|\\bCurrent\\b|\\bcurrent\\b'\n",
    "\n",
    "def date_search(resume):\n",
    "    \n",
    "    ans = []\n",
    "\n",
    "    # find all the date occurrence based on the regular expression\n",
    "    res = list(re.finditer(pattern, resume))\n",
    "\n",
    "    if len(res) > 1:\n",
    "        for ele in res:\n",
    "            # this is to eradicate the results of having only year but without month\n",
    "            if len(ele.group().strip()) > 5:\n",
    "                ans.append([ele.start(), ele.end(), ele.group().strip()])\n",
    "\n",
    "    # Convert \"present\" and \"current\" to today's date\n",
    "    for ele in ans:\n",
    "        if ele[2].lower() == 'present' or ele[2].lower() == 'current':\n",
    "            today = pd.to_datetime('today').date()\n",
    "            ele[2] = today\n",
    "        else:\n",
    "            day = pd.to_datetime(ele[2]).date()\n",
    "            ele[2] = day\n",
    "    \n",
    "    # all the date results are given in the form of [datetime_start_index, datetime_end_index, datetime]\n",
    "    return ans\n",
    "\n",
    "date_list = date_search(resume_string)\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: [(981, 1066)], 4: [(1996, 3608)], 3: [(3635, -1)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def experience_tagging(date_list):\n",
    "    i = 1\n",
    "    cleaned_section = {}\n",
    "    while i < len(date_list):\n",
    "        cur = date_list[i]\n",
    "        prev = date_list[i-1]\n",
    "\n",
    "        if cur[0] < prev[1] + 10:\n",
    "            # Taking ceiling of the year of experience\n",
    "            key = ((cur[2] - prev[2]).days // 365) + 1\n",
    "            if i < len(date_list) - 1:\n",
    "                # if there is another date that appears later, then the section will be until the start index of the next date\n",
    "                until = date_list[i+1][0]\n",
    "            else:\n",
    "                until = -1\n",
    "            frm = cur[1]+1\n",
    "\n",
    "            # Multiplie projects with same year of experience, we do 'chaining' here\n",
    "            if key in cleaned_section:\n",
    "                cleaned_section[key].append((frm, until))\n",
    "            else:\n",
    "                cleaned_section[key] = [(frm, until)]\n",
    "            i += 2\n",
    "        else:\n",
    "            # ignore the current date, possibly it is useless\n",
    "            i += 1\n",
    "\n",
    "    return cleaned_section\n",
    "\n",
    "experience_tagging(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduction': 4,\n",
       " 'environmental quality': 4,\n",
       " 'teamwork': 4,\n",
       " 'management': 4,\n",
       " 'poultry': 4,\n",
       " 'strong work ethic': 4,\n",
       " 'business administration': 3,\n",
       " 'coaching': 3,\n",
       " 'conflict resolution': 3,\n",
       " 'workplace safety': 3,\n",
       " 'mediation': 3,\n",
       " 'organizational development': 3,\n",
       " 'session': 3,\n",
       " 'ada': 3,\n",
       " 'labor law': 1,\n",
       " 'policy development': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def skills_experience_level_identification(resume):\n",
    "    res = {}\n",
    "    date_list = date_search(resume)\n",
    "    experience_sec = experience_tagging(date_list)\n",
    "\n",
    "    for key in experience_sec:\n",
    "        for start, end in experience_sec[key]:\n",
    "            skills_list = extract_skills(resume[start: end], skills_api, True)\n",
    "            for ele in skills_list:\n",
    "                if ele not in res:\n",
    "                    res[ele] = key\n",
    "                else:\n",
    "                    res[ele] = max(key, res[ele])\n",
    "\n",
    "    skills_list = extract_skills(resume, skills_api, True)\n",
    "    for ele in skills_list:\n",
    "        if ele not in res:\n",
    "            res[ele] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "skills_experience_level_identification(resume_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c64ed8c669f425495e1d5a2fc914d4b3fc5bfa5a9e06f8663c5733f83a88b100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
