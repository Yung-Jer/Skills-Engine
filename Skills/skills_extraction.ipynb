{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "import aspose.words as aw\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Tokenize words only with the whitespace rule\n",
    "# N-grams will no longer be treated as 'N' and '-grams'\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # these must come first\n",
    "    txt = re.sub('b\\.\\S*', '', txt) # remove all bachelor qualifications\n",
    "    txt = re.sub('m\\.\\S*', '', txt) # remove all master qualifications\n",
    "\n",
    "    txt = re.sub('[^a-zA-Z]', ' ', txt) # Remove non-English charcters\n",
    "    txt = re.sub('http\\S+\\s*', ' ', txt)  # remove URLs\n",
    "    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc\n",
    "    txt = re.sub('#\\S+', '', txt)  # remove hashtags\n",
    "    txt = re.sub('@\\S+', ' ', txt)  # remove mentions\n",
    "    txt = re.sub('\\s+', ' ', txt)  # remove extra whitespace\n",
    "\n",
    "    # tokenize word\n",
    "    txt = nlp(txt)\n",
    "\n",
    "    # remove stop words and lemmatization\n",
    "    txt = [token.lemma_ for token in txt if not token.is_stop]\n",
    "\n",
    "    return ' '.join(txt)\n",
    "\n",
    "# STOP = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def topic_modelling(resume_text, num_words=10, min_prob = 10**-2):\n",
    "    doc_clean = [resume_text.split()]   \n",
    "    \n",
    "    # term dictionary\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_mat = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    # latent dirichlet allocation model \n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    ldamodel = Lda(doc_term_mat, num_topics=1, id2word = dictionary, passes=50)\n",
    "    \n",
    "    # Return only the topic words that have the probability of larger than .01\n",
    "    return [token for token, prob in ldamodel.show_topic(0, topn=num_words) if prob > min_prob ]\n",
    "\n",
    "def n_grams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the skills from EMSI Skills API dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>infoUrl</th>\n",
       "      <th>name</th>\n",
       "      <th>type.id</th>\n",
       "      <th>type.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KS126XS6CQCFGC3NG79X</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XS6CQC...</td>\n",
       "      <td>.net assemblies</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ES50D03AC9CFC1A0BC93</td>\n",
       "      <td>https://skills.emsidata.com/skills/ES50D03AC9C...</td>\n",
       "      <td>.net development</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>KS1200B62W5ZF38RJ7TD</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS1200B62W5...</td>\n",
       "      <td>.net framework</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KS126XW78QJCF4TRV2X7</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XW78QJ...</td>\n",
       "      <td>.net framework 1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>KS126XY68BNKXSBSLPYS</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XY68BN...</td>\n",
       "      <td>.net framework 3</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    id  \\\n",
       "0           0  KS126XS6CQCFGC3NG79X   \n",
       "1           1  ES50D03AC9CFC1A0BC93   \n",
       "2           2  KS1200B62W5ZF38RJ7TD   \n",
       "3           3  KS126XW78QJCF4TRV2X7   \n",
       "4           4  KS126XY68BNKXSBSLPYS   \n",
       "\n",
       "                                             infoUrl              name  \\\n",
       "0  https://skills.emsidata.com/skills/KS126XS6CQC...   .net assemblies   \n",
       "1  https://skills.emsidata.com/skills/ES50D03AC9C...  .net development   \n",
       "2  https://skills.emsidata.com/skills/KS1200B62W5...    .net framework   \n",
       "3  https://skills.emsidata.com/skills/KS126XW78QJ...  .net framework 1   \n",
       "4  https://skills.emsidata.com/skills/KS126XY68BN...  .net framework 3   \n",
       "\n",
       "  type.id          type.name  \n",
       "0     ST1  Specialized Skill  \n",
       "1     ST1  Specialized Skill  \n",
       "2     ST1  Specialized Skill  \n",
       "3     ST1  Specialized Skill  \n",
       "4     ST1  Specialized Skill  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_excel(\"all_skills_emsi.xlsx\")\n",
    "# remove all the additional descriptions in round brackets\n",
    "skills['name'] = skills['name'].apply(lambda x: re.sub(\"\\W\\(.*?\\)\",\"\",x))\n",
    "skills['name'] = skills['name'].apply(lambda x: x.lower())\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the skills into a big hashset\n",
    "skills_api = set(skills['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text, skills_api, clean=True):\n",
    "    if clean == True:\n",
    "        resume_text = preprocess(resume_text)\n",
    "        \n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # all the resume skills will be saved here\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if nlp(token)[0].tag_ != 'VBN':\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for noun_chunks (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for bigrams that SpaCy missed in the noun_chuncks\n",
    "    for token in n_grams(tokens, 2):\n",
    "        token = token.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for trigrams that SpaCy missed in the noun_chuncks\n",
    "    for token in n_grams(tokens, 3):\n",
    "        token = token.lower().strip()\n",
    "        skillset.append(token)\n",
    "    \n",
    "    # get only the unique skills in lowercase\n",
    "    skillset = set([i for i in set([i.lower() for i in skillset])])\n",
    "\n",
    "    return skillset.intersection(skills_api)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read any resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = aw.Document(\"34740556.pdf\")\n",
    "resume_string = resume.to_string(aw.SaveFormat.TEXT).split('\\r\\n')\n",
    "resume_string = ' '.join(resume_string[1:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SENIOR HR BUSINESS PARTNER Summary Human Resources Professional with 8 years of experience in human resources and recruiting. Expertise in Employee Relations and Recruiting. Highly driven to achieve company goals. Highlights · HUMAN RESOURCES MANAGER · Extensive background in HR Business Partner affairs, including experience in, staff development, mediation, conflict resolution, benefits and compensation, HR records management, HR policies development and legal compliance. · Demonstrated success in negotiating win-win compromises, developing teambuilding programs, and writing policies, job descriptions and management reports. · HR SKILLS · Employment Law · FMLA/ADA/EEO/WC · HR Policies & Procedures *Staff Recruitment & Retention · Employee Relations · Benefits Administration *Orientation & On-Boarding  Training & Development  Organizational Development  MS Office (Word, Excel, PowerPoint, Access, Outlook) Experience Senior HR Business Partner  August 2013 to Current Company Name ï¼ Cit'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_string[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada',\n",
       " 'business administration',\n",
       " 'coaching',\n",
       " 'conflict resolution',\n",
       " 'environmental quality',\n",
       " 'labor law',\n",
       " 'management',\n",
       " 'mediation',\n",
       " 'organizational development',\n",
       " 'policy development',\n",
       " 'poultry',\n",
       " 'reduction',\n",
       " 'session',\n",
       " 'strong work ethic',\n",
       " 'teamwork',\n",
       " 'workplace safety'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(resume_text=resume_string, skills_api=skills_api, clean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is actually assuming all the skills have the same level of experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c64ed8c669f425495e1d5a2fc914d4b3fc5bfa5a9e06f8663c5733f83a88b100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
