{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "import aspose.words as aw\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Tokenize words only with the whitespace rule\n",
    "# N-grams will no longer be treated as 'N' and '-grams'\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # these must come first\n",
    "    txt = re.sub('b\\.\\S*', '', txt) # remove all bachelor qualifications\n",
    "    txt = re.sub('m\\.\\S*', '', txt) # remove all master qualifications\n",
    "\n",
    "\n",
    "    txt = re.sub(\"'\", \"\", txt) # Remove apostrophe\n",
    "    txt = re.sub(\"’\", \"\", txt) # Remove apostrophe\n",
    "    txt = re.sub('http\\S+\\s*', ' ', txt)  # remove URLs\n",
    "    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc\n",
    "    txt = re.sub('#\\S+', '', txt)  # remove hashtags\n",
    "    txt = re.sub('@\\S+', ' ', txt)  # remove mentions\n",
    "    txt = re.sub('[^a-zA-Z]', ' ', txt) # Remove non-English charcters\n",
    "    txt = re.sub('\\s+', ' ', txt)  # remove extra whitespace\n",
    "\n",
    "    # tokenize word\n",
    "    txt = nlp(txt)\n",
    "\n",
    "    # remove stop words and lemmatization\n",
    "    txt = [token.text for token in txt if not token.is_stop]\n",
    "\n",
    "    return ' '.join(txt)\n",
    "\n",
    "# STOP = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Suggestion:\n",
    "# def topic_modelling(resume_text, num_words=10, min_prob = 10**-2):\n",
    "#     doc_clean = [resume_text.split()]   \n",
    "    \n",
    "#     # term dictionary\n",
    "#     dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#     # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "#     doc_term_mat = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#     # latent dirichlet allocation model \n",
    "#     Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#     ldamodel = Lda(doc_term_mat, num_topics=1, id2word = dictionary, passes=50)\n",
    "    \n",
    "#     # Return only the topic words that have the probability of larger than .01\n",
    "#     return [token for token, prob in ldamodel.show_topic(0, topn=num_words) if prob > min_prob ]\n",
    "\n",
    "def n_grams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the skills from EMSI Skills API dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>infoUrl</th>\n",
       "      <th>name</th>\n",
       "      <th>type.id</th>\n",
       "      <th>type.name</th>\n",
       "      <th>lemmatized_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KS126XS6CQCFGC3NG79X</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XS6CQC...</td>\n",
       "      <td>.net assemblies</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net assembly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ES50D03AC9CFC1A0BC93</td>\n",
       "      <td>https://skills.emsidata.com/skills/ES50D03AC9C...</td>\n",
       "      <td>.net development</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>KS1200B62W5ZF38RJ7TD</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS1200B62W5...</td>\n",
       "      <td>.net framework</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KS126XW78QJCF4TRV2X7</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XW78QJ...</td>\n",
       "      <td>.net framework 1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>KS126XY68BNKXSBSLPYS</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XY68BN...</td>\n",
       "      <td>.net framework 3</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    id  \\\n",
       "0           0  KS126XS6CQCFGC3NG79X   \n",
       "1           1  ES50D03AC9CFC1A0BC93   \n",
       "2           2  KS1200B62W5ZF38RJ7TD   \n",
       "3           3  KS126XW78QJCF4TRV2X7   \n",
       "4           4  KS126XY68BNKXSBSLPYS   \n",
       "\n",
       "                                             infoUrl              name  \\\n",
       "0  https://skills.emsidata.com/skills/KS126XS6CQC...   .net assemblies   \n",
       "1  https://skills.emsidata.com/skills/ES50D03AC9C...  .net development   \n",
       "2  https://skills.emsidata.com/skills/KS1200B62W5...    .net framework   \n",
       "3  https://skills.emsidata.com/skills/KS126XW78QJ...  .net framework 1   \n",
       "4  https://skills.emsidata.com/skills/KS126XY68BN...  .net framework 3   \n",
       "\n",
       "  type.id          type.name   lemmatized_name  \n",
       "0     ST1  Specialized Skill     .net assembly  \n",
       "1     ST1  Specialized Skill  .net development  \n",
       "2     ST1  Specialized Skill    .net framework  \n",
       "3     ST1  Specialized Skill  .net framework 1  \n",
       "4     ST1  Specialized Skill  .net framework 3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_excel(\"all_skills_emsi.xlsx\")\n",
    "# remove all the additional descriptions in round brackets\n",
    "skills['name'] = skills['name'].apply(lambda x: re.sub(\"\\W\\(.*?\\)\",\"\",x))\n",
    "skills['name'] = skills['name'].apply(lambda x: x.lower())\n",
    "skills['lemmatized_name'] = skills['name'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the skills into a big hashset\n",
    "skills_api = set(skills['name']).union(set(skills['lemmatized_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text, skills_api, clean=True):\n",
    "    if clean == True:\n",
    "        resume_text = preprocess(resume_text)\n",
    "        \n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # all the resume skills will be saved here\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        skillset.append(token)\n",
    "    \n",
    "    # check for noun_chunks (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for N-grams that SpaCy missed in the noun_chuncks\n",
    "    for n in range(2, 10):\n",
    "        for token in n_grams(tokens, n):\n",
    "            token = token.lower().strip()\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # get only the unique skills in lowercase\n",
    "    skillset = set([i for i in set([i.lower() for i in skillset])])\n",
    "\n",
    "    return skillset.intersection(skills_api)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read any resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = aw.Document(\"31605080.pdf\")\n",
    "resume_string = resume.to_string(aw.SaveFormat.TEXT).split('\\r\\n')\n",
    "resume_string = ' '.join(resume_string[1:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEEK SQUAD AGENT Professional Profile IT support specialist with experience across multiple disciplines including technical support, customer service, computer repair, and military service. I am hard working, willing to learn, team oriented, and comfortable working independently as well. Qualifications Windows / Mac / IOS / Android Technical Support Hardware & Software Maintenance User Training Malware Detection & Removal Customer Service Entry Level Active Directory & Ticketing Problem Solving & Research Experience Company Name August 2014 to October 2016 Geek Squad Agent City , State · Provided technical support in person and over the phone. · Performed hardware and software installation and repair. · Refurbished and setup PCs and peripheral devices. Company Name January 2013 to January 2014 Shipping & Receiving Associate City , State · Performed shipping and receiving of product. · Assisted with inventory management. · General logistics and warehouse duties. Company Name January 200'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_string[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active directory',\n",
       " 'android',\n",
       " 'aviation',\n",
       " 'computer repair',\n",
       " 'customer service',\n",
       " 'information technology',\n",
       " 'installation',\n",
       " 'inventory management',\n",
       " 'level',\n",
       " 'logistics',\n",
       " 'malware detection',\n",
       " 'management',\n",
       " 'marshalling',\n",
       " 'military service',\n",
       " 'peripheral devices',\n",
       " 'problem solving',\n",
       " 'profile',\n",
       " 'repair',\n",
       " 'research',\n",
       " 'research experience',\n",
       " 'safe',\n",
       " 'software installation',\n",
       " 'software maintenance',\n",
       " 'team oriented',\n",
       " 'technical support',\n",
       " 'troubleshooting',\n",
       " 'warehouse'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(resume_text=resume_string, skills_api=skills_api, clean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is actually assuming all the skills have the same level of experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c64ed8c669f425495e1d5a2fc914d4b3fc5bfa5a9e06f8663c5733f83a88b100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
