{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy\n",
    "import aspose.words as aw\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Tokenize words only with the whitespace rule\n",
    "# N-grams will no longer be treated as 'N' and '-grams'\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # these must come first\n",
    "    txt = re.sub('b\\.\\S*', '', txt) # remove all bachelor qualifications\n",
    "    txt = re.sub('m\\.\\S*', '', txt) # remove all master qualifications\n",
    "\n",
    "\n",
    "    txt = re.sub(\"'\", \"\", txt) # Remove apostrophe\n",
    "    txt = re.sub(\"â€™\", \"\", txt) # Remove apostrophe\n",
    "    txt = re.sub('http\\S+\\s*', ' ', txt)  # remove URLs\n",
    "    txt = re.sub('RT|cc', ' ', txt)  # remove RT and cc\n",
    "    txt = re.sub('#\\S+', '', txt)  # remove hashtags\n",
    "    txt = re.sub('@\\S+', ' ', txt)  # remove mentions\n",
    "    txt = re.sub('[^a-zA-Z]', ' ', txt) # Remove non-English charcters\n",
    "    txt = re.sub('\\s+', ' ', txt)  # remove extra whitespace\n",
    "\n",
    "    # tokenize word\n",
    "    txt = nlp(txt)\n",
    "\n",
    "    # remove stop words and lemmatization\n",
    "    txt = [token.text for token in txt if not token.is_stop]\n",
    "\n",
    "    return ' '.join(txt)\n",
    "\n",
    "# STOP = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Suggestion:\n",
    "# def topic_modelling(resume_text, num_words=10, min_prob = 10**-2):\n",
    "#     doc_clean = [resume_text.split()]   \n",
    "    \n",
    "#     # term dictionary\n",
    "#     dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#     # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "#     doc_term_mat = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "#     # latent dirichlet allocation model \n",
    "#     Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#     ldamodel = Lda(doc_term_mat, num_topics=1, id2word = dictionary, passes=50)\n",
    "    \n",
    "#     # Return only the topic words that have the probability of larger than .01\n",
    "#     return [token for token, prob in ldamodel.show_topic(0, topn=num_words) if prob > min_prob ]\n",
    "\n",
    "def n_grams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the skills from EMSI Skills API dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>infoUrl</th>\n",
       "      <th>name</th>\n",
       "      <th>type.id</th>\n",
       "      <th>type.name</th>\n",
       "      <th>lemmatized_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>KS126XS6CQCFGC3NG79X</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XS6CQC...</td>\n",
       "      <td>.net assemblies</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net assembly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ES50D03AC9CFC1A0BC93</td>\n",
       "      <td>https://skills.emsidata.com/skills/ES50D03AC9C...</td>\n",
       "      <td>.net development</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>KS1200B62W5ZF38RJ7TD</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS1200B62W5...</td>\n",
       "      <td>.net framework</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>KS126XW78QJCF4TRV2X7</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XW78QJ...</td>\n",
       "      <td>.net framework 1</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>KS126XY68BNKXSBSLPYS</td>\n",
       "      <td>https://skills.emsidata.com/skills/KS126XY68BN...</td>\n",
       "      <td>.net framework 3</td>\n",
       "      <td>ST1</td>\n",
       "      <td>Specialized Skill</td>\n",
       "      <td>.net framework 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    id  \\\n",
       "0           0  KS126XS6CQCFGC3NG79X   \n",
       "1           1  ES50D03AC9CFC1A0BC93   \n",
       "2           2  KS1200B62W5ZF38RJ7TD   \n",
       "3           3  KS126XW78QJCF4TRV2X7   \n",
       "4           4  KS126XY68BNKXSBSLPYS   \n",
       "\n",
       "                                             infoUrl              name  \\\n",
       "0  https://skills.emsidata.com/skills/KS126XS6CQC...   .net assemblies   \n",
       "1  https://skills.emsidata.com/skills/ES50D03AC9C...  .net development   \n",
       "2  https://skills.emsidata.com/skills/KS1200B62W5...    .net framework   \n",
       "3  https://skills.emsidata.com/skills/KS126XW78QJ...  .net framework 1   \n",
       "4  https://skills.emsidata.com/skills/KS126XY68BN...  .net framework 3   \n",
       "\n",
       "  type.id          type.name   lemmatized_name  \n",
       "0     ST1  Specialized Skill     .net assembly  \n",
       "1     ST1  Specialized Skill  .net development  \n",
       "2     ST1  Specialized Skill    .net framework  \n",
       "3     ST1  Specialized Skill  .net framework 1  \n",
       "4     ST1  Specialized Skill  .net framework 3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_excel(\"all_skills_emsi.xlsx\")\n",
    "# remove all the additional descriptions in round brackets\n",
    "skills['name'] = skills['name'].apply(lambda x: re.sub(\"\\W\\(.*?\\)\",\"\",x))\n",
    "skills['name'] = skills['name'].apply(lambda x: x.lower())\n",
    "skills['lemmatized_name'] = skills['name'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the skills into a big hashset\n",
    "skills_api = set(skills['name']).union(set(skills['lemmatized_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(resume_text, skills_api, clean=True):\n",
    "    if clean == True:\n",
    "        resume_text = preprocess(resume_text)\n",
    "        \n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # all the resume skills will be saved here\n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        skillset.append(token)\n",
    "    \n",
    "    # check for noun_chunks (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        skillset.append(token)\n",
    "\n",
    "    # check for N-grams that SpaCy missed in the noun_chuncks\n",
    "    for n in range(2, 10):\n",
    "        for token in n_grams(tokens, n):\n",
    "            token = token.lower().strip()\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # get only the unique skills in lowercase\n",
    "    skillset = set([i for i in set([i.lower() for i in skillset])])\n",
    "\n",
    "    return skillset.intersection(skills_api)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read any resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = aw.Document(\"31605080.pdf\")\n",
    "resume_string = resume.to_string(aw.SaveFormat.TEXT).split('\\r\\n')\n",
    "resume_string = ' '.join(resume_string[1:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GEEK SQUAD AGENT Professional Profile IT support specialist with experience across multiple disciplines including technical support, customer service, computer repair, and military service. I am hard working, willing to learn, team oriented, and comfortable working independently as well. Qualifications Windows / Mac / IOS / Android Technical Support Hardware & Software Maintenance User Training Malware Detection & Removal Customer Service Entry Level Active Directory & Ticketing Problem Solving & Research Experience Company Name August 2014 to October 2016 Geek Squad Agent City , State Â· Provided technical support in person and over the phone. Â· Performed hardware and software installation and repair. Â· Refurbished and setup PCs and peripheral devices. Company Name January 2013 to January 2014 Shipping & Receiving Associate City , State Â· Performed shipping and receiving of product. Â· Assisted with inventory management. Â· General logistics and warehouse duties. Company Name January 200'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_string[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active directory',\n",
       " 'android',\n",
       " 'aviation',\n",
       " 'computer repair',\n",
       " 'customer service',\n",
       " 'information technology',\n",
       " 'installation',\n",
       " 'inventory management',\n",
       " 'level',\n",
       " 'logistics',\n",
       " 'malware detection',\n",
       " 'management',\n",
       " 'marshalling',\n",
       " 'military service',\n",
       " 'peripheral devices',\n",
       " 'problem solving',\n",
       " 'profile',\n",
       " 'repair',\n",
       " 'research',\n",
       " 'research experience',\n",
       " 'safe',\n",
       " 'software installation',\n",
       " 'software maintenance',\n",
       " 'team oriented',\n",
       " 'technical support',\n",
       " 'troubleshooting',\n",
       " 'warehouse'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(resume_text=resume_string, skills_api=skills_api, clean=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is actually assuming all the skills have the same level of experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c64ed8c669f425495e1d5a2fc914d4b3fc5bfa5a9e06f8663c5733f83a88b100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
